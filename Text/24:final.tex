% -*- LaTeX -*-

\documentstyle[10pt]{article}
\newcommand{\reg}[1]{{\sc \%#1}}

\topmargin      -1.5cm
\oddsidemargin   0.0cm
\evensidemargin  0.0cm
\textwidth      6.5in
\textheight     9.0in
\parindent       0.0cm
\parskip         0.4cm

\title{Larceny Note \#24: \\
A Retrospective on the Bootstrapping and Testing of Larceny}
\author{Lars Thomas Hansen}
\begin{document}
\maketitle

\begin{abstract}
This note started life as a writeup I did for my graduate level software
engineering class in the Spring of 1992. It is the only form of written
doucmentation which currently exists on the history of the Larceny project,
and as such it is incomplete and probably inaccurate.

The moral of the story is to keep a log or a diary of at least the major
events; you'd be surprised how interesting they can be in retrospect.
\end{abstract}

\begin{flushright}
\em
Tragedy, sir. Deaths and disclosures, universal \\
and particular, denouements both unexpected and \\
inexorable, transvestite melodrama on all levels \\
including the suggestive. We transport you into a \\
world of intrigue and illusion\ldots \\
\medskip
-- The Player \\
``Rosenkrantz and Guildenstern are Dead''
\end{flushright}

\section{Introduction}

``Larceny'' is a native-code Scheme system for the SPARC architecture,
designed and implemented mainly by Will Clinger and myself. This report
chronicles how the system was bootstrapped and tested.

The details of the bootstrap/test process are interesting because the parts
of the system being tested were either written in assembly language, were
generated by our own assembler (and hence the bugs appeared in the generated
machine code), or dealt with the bit-level representations of the system and
its data structures; in other words, the level of abstraction was unusually
low. In addition, to solve some of the problems encountered required the
programmer to have knowledge about many parts of the system.

The current system  consists of roughly 4,500 lines of handcoded SPARC
assembly code, 2,000 lines of C code, 9,000 lines of mostly portable Scheme
code which goes into the run-time libraries (the non-portable parts deal
with the representations of Scheme objects like numbers and I/O ports), and
9,000 lines of Scheme code which is part of the development system (portable
compiler, SPARC code generator, and SPARC assembler). The development system
code will eventually be moved into the run-time library to create a
standalone system; current development work is being done under Chez Scheme.

While much of the library Scheme code was taken straight from the
MacScheme libraries and was already debugged, some system level Scheme code
had to be written or rewritten for Larceny, and to a great extent this new code
had to be debugged while the run-time system was still extremely unstable.
Much of the development system was also being debugged while
we were debugging the
run-time system.

In summary, debugging was performed on a number of linguistic levels: machine
language, assembly language, C, and Scheme, and because of the relatively
tight integration of the program modules, the context for debugging was
large.

Section 2 presents the bootstrapping process. Section 3 presents an overview
of the ``tools of the trade:'' the development and debugging tools used.
Section 4 details the testing of the garbage collector and memory management
system. Section 5 presents some issues in debugging the assembler. Section 6
covers the generic arithmetic system, and section 7 concludes the paper and
summarizes our experiences.

\section{Bootstrapping}

Larceny was bootstrapped in several phases.  I got suggestions for data
structure layouts and some hints about a reasonable garbage collector design
from Will during winter term of 1991, and work commenced in March of
that year.

\subsection{Data Layouts, the Garbage Collector, and the Calling Conventions}

As soon as the primitive data structure layouts had been cast in stone, a
preliminary version of the garbage collector was implemented, and module
testing was performed on the garbage collector to verify that it was correct
in a rudimentary way. The garbage collector was implemented carefully over
a period of several weeks and hand-checked extensively before being tested.

Around this time we also designed the calling conventions for Scheme
procedures and tested these conventions for performance by hand-coding some
small programs and linking them into the very rudimentary run-time system.
After some tuning, the benchmark numbers were acceptable, and we felt
confident that the calling conventions would be sufficient for a
high-performance system.

\subsection{The Compiler, the Assembler, and the Heap Dumper}

When the collector was stable, the C and assembly language ccode procedures
which allocate memory and interface to the garbage collector were
implemented and tested.

It is worth noting that the internal calling conventions in some of the
hand-coded assembly language routines were already beginning to be a problem
when we got to this stage. The conventions were designed for speed and
restricted the call depth to two in most cases. This call depth is just
exactly enough, but the restriction has caused problems, and it was
definitely a mistake to not use more general (although perhaps slower)
calling conventions in the assembly code.  It was also a mistake to not
correct the problem at this early stage.

Anyway, at this point the system could in theory load and execute heap
images.  However, no heap images existed because the compiler and assembler
were not available.  We therefore commenced work on the assembler, reasoning
that we could compile under MacScheme and assemble locally under Chez Scheme
on the SPARC.

The assembler includes a pass which translates the output of the compiler
(MacScheme Assembly Language) to SPARC assembly language in a suitable
format for input to the assembler proper. When this pass was implemented, it
turned out that due to shoddy communication between the developers, the
MacScheme instruction set made assumptions not sanctioned by the calling
conventions as designed, and vice versa. We therefore had to reiterate and
redesign the calling conventions to fit the new paradigm. This was less
painful than one might think, and performance was not adversely affected.

The compiler was ported to run under Chez Scheme only when compiling on the
Macintosh became unbearable.  The port uncovered some portability problems
because MacScheme (under which the compiler was developed) and Chez Scheme
have incompatible definitions of some common but non-standard Scheme
procedures, in particular {\tt error} and {\tt sort}.  These problems were
solved by writing replacement procedures for the Chez Scheme procedures
rather than changing the compiler. All in all the porting process was
relatively painless. The port introduced a nasty bug, however, which only
showed up much later. When I got the files for the compiler, one file was
missing. Since Will was nowhere to be found and the missing file simply
implemented set operations (union, difference, intersection), I decided to
write a replacement. The replacement had a bug, however. When this bug
finally showed up, nobody remembered that a replacement had been written and
it took several months before the bug was tracked down.  The moral is to
keep a log of changes made to the system during the port.

When the compiler and assembler were running (and the assembler was still
undebugged), a disassembler and a heap dumper were implemented. The
disassembler takes an object code file as produced by the assembler and
translates it back to SPARC assembly language. The heap dumper takes a
number of object code files and creates a loadable heap: a binary image of
the heap which can be loaded into the executable.

While we could in principle have used the UNIX SPARC assembler to assemble
our programs, the advantage of having our own is significant because we can
incorporate optimizations into it, and it can run in our development system
without any magic. More importantly, when the development system becomes
part of the interactive programming environment in Larceny, we will need an
assembler. It was convenient to write one at this point, even if it was a
source of quite a few bugs.

The heap dumper was tested in a very rudimentary way to make sure that it
generated heaps which looked correct. The heap dumper was well trusted
because it had been developed to be clear and understandable rather than
fast, and it proved to be both robust and easily extended.

\subsection{Initial Runs}

We now had a development system which in principle was capable of generating
a heap image which could be loaded into a running system. The only problem
was that the development system was a large chunk of code (about 8,000 lines
of Scheme), and it was mostly untested. As a first test case, therefore, we
compiled the Fibonacci function and dumped a heap containing some dummy
initialization code and a call to {\tt fib}. Amazingly, it ran without many
problems, and we set our sights higher: to get the Scheme {\tt write}
procedure up and running.  We knew that armed with a working {\tt write}
procedure, debugging would be much easier, as we could insert output
statements in the code to get status information.

Whereas the {\tt write} procedure itself and most of the support code it
required had already been written (it was taken from the MacScheme library), the
low-level I/O system had been neither designed nor implemented. Since we
wanted the I/O system to handle not only file streams but perhaps string
streams and who knows what else, some effort had to go into its design to
get it flexible enough to handle whatever problems we might eventually throw
at it.  The I/O system was written almost entirely in Scheme, with some
very-low-level hooks directly into UNIX to do file operations.

\subsection{The I/O System}

Testing the I/O system unearthed a great number of bugs, as we had expected.
First, most of the code generation procedures in the assembler had not been
exercised by the Fibonacci function, so a number of code generator and
assembler bugs were found.

Second, parts of the MacScheme libraries were incompatible with our heap
dumper, resulting in circular references in the initialization phase when a
heap was being loaded: procedure A would need to call procedure B to be
initialized, but procedure B was either not initialized yet or it would need
to call something which would call procedure A. These errors were dealt with
by postponing the initialization to a later time or by changing the load
order of object modules. Another incompatibility was the difference in
low-level primitives between Larceny and MacScheme. These differences were
ultimately due to different basic representations, and some code had to be
rewritten to work in Larceny.

Third, the way the I/O system was coded (as a very large {\tt let} statement
which assigned locally defined procedures to global variables) uncovered a
``bug'' in the compiler. I use the quotes because the instruction emitted
by the compiler had an operand which from my point of view was nonsensical,
but which had meaning in the context. Specifically, there is a 
{\tt SAVE} instruction which creates a stack frame and sets up a return 
address in the frame:

\begin{verbatim}
        SAVE    Ln, k
\end{verbatim}

This instruction saves registers {\tt REG0} through {\tt REGk} in a fresh
stack frame. The problem was that in this specific case, {\tt k} was 50, yet
there are only 32 registers in the virtual machine. The intent of the
compiler was to allocate a very large frame in which it could put temporary
variables. This behavior was not to be found in the spec for the instruction
set, but since a compiler fix was not forthcoming for various reasons, I
solved the problem by making the compiler complain about all such
anomalities, and then re-coding any offending code. Later, as a still
temporary measure, we extended the definition of the instruction to deal
with such cases to allow arbitrary programs to work.

\subsection{Really Low-level Debugging}

Around the time of the implementation of the I/O system, Larceny acquired a
very-low-level debugger, known as the ``local debugger''. The local debugger
would be the last resort of the run-time system when an error was detected.
The exception handler stub would call the local debugger with a numeric
argument indicating the general class of the error, and the local debugger
would print a message and present the user with a command interface which
allowed the inspection of machine registers and the state of memory. This
interface, primitive though as it was, was very helpful in the early stages
of system development in diagnosing post-mortem conditions in order to track
down bugs.

We also implemented a {\tt break} procedure at the Scheme level. A procedure
could call {\tt break} and the program would enter the local debugger. This
way, we could inspect the state of the machine even when the
execution appeared to be going well. A command line switch to the Larceny
executable enabled the breakpoints.
Another debugging tool used for a while was a selective breakpoint facility.
A command line switch would enable certain breakpoints by {\em dynamic
breakpoint count}, i.e. the command line switch \verb+-B 5+ would break at
the fifth dynamically occuring breakpoint. This was useful to an extent
early on, but later lost all utility as programs got bigger.

With the I/O system up and running, the {\tt write} procedure was integrated
into the system in no time, with occasional patches to remove or change code
which was completely MacScheme specific (as outlined above). We are now at
the end of 1991.

\subsection{The Reader}

The {\tt read} procedure is one of the hairiest pieces of code in the Scheme
library. The {\tt read} procedure also exercises a large number of the
library procedures. Concluding that it is nice to be able to interact with
one's system, and hoping that by getting the reader running we would get
most of the rest of the library running, we commenced on getting the reader
to work. This turned out to be uneventful.  Except for some truly ancient
code in the reader (Scheme syntax I had never even heard of) which did not
work well with the Chez Scheme hosted compiler, the reader was integrated into
the system in short time.  A little command loop was written to provide a
rudimentary interactive debugging facility at this point.

\subsection{The Stack Cache Bug}

Since we were now starting to run interesting programs which exercised
the run-time system, bugs in the system started to appear. 

The most serious bug was related to the management of the stack cache.  The
stack cache is what the name implies: a fixed-size area of memory in which
stack frames are created.\footnote{The motivation for the stack cache is
simple: it makes it straightforward to implement efficient {\tt call/cc}
while still having adequate performance in the rest of the system.} The
stack cache may overflow during deep recursions, and if so, the cache is
spilled to the heap and reused for new frames. Eventually, the program
returns through the saved frames, and if the cache was spilled on the way
``down'', it will eventually underflow on the way ``up''.  When the cache
underflows, a frame must be restored from the heap into the cache. In order
to improve performance, stack cache underflow detection is implemented with
a dummy frame at the bottom of the stack. The return address in this dummy
frame points to the underflow handler, and so the Scheme program need never
explicitly check for underflow -- it simply returns through the dummy frame
into the handler, which in turn restores a frame and jumps to the return
address in the restored frame.

Now, it turns out that the compiler would sometimes generate two stackframe
creations ({\tt SAVE} instructions) in a row without an intervening frame pop.
This violates (in subtle ways) the invariants in the cheap underflow
detection scheme. We therefore got rather awkward behavior from some
programs which caused stack cache flushes.  The problem is really in the
compiler, but it was fixed temporarily in the code generator by inserting
explicit checks for underflow, slowing down the system somewhat. A later
compiler will fix this problem.


\subsection{The Garbage Collector Bug}

Another major bug in the run-time system was in the garbage collector.  Its
implementer (me) had erroneously assumed that if it is not the case that $a
<= x <= b$ for some particular $x$, $a$, and $b$, then it is the case that
$c <= x <= d$ for some particular $c$ and $d$. Or, put another way, that a
pointer not pointing into memory area $A$ by necessity must point into area
$B$. This assumption is wrong in general and in Larceny in particular, and
caused no end of problems during garbage collection, because
intergenerational pointers were not being tracked correctly: some pointers
ended up pointing to old objects in a (momentarily) disused area of memory.
At the next collection, new values were copied into this disused area, and
the pointers which had been left pointing into that area were now pointing
to random data.  While a minor thing to fix, it took several days to track
down.

\subsection{CIS 561 and the Single Stepper}

Will taught the graduate compiler class in the winter of 1992. The language
for which the students wrote compilers, Quirk 6, is a statically typed
language with first class procedures and data objects. Its implementation
requires a run-time system with at least a garbage collector, and Will
decided that because Larceny was sufficiently stable, the students would
be generating MacScheme assembly language and then use the Larceny
 development system
to generate heaps which they would finally run under the Larceny run-time
system.

The students found it somewhat intimidating to debug their programs on the
level of SPARC assembly language, which they did not know, and it was also
difficult to relate the SPARC code to the MacScheme instructions generated
by their compilers. I therefore added a very simple single stepper to
Larceny, which would allow interactive debugging at the level of the
MacScheme assembly language instruction. While this single stepper was not
useful in the further debugging of Larceny, it has potential for being very
useful during a port of the system to a new architecture, because if the
program crashes while stepping through it, we can pretty much identify the
offending instruction. The single stepping facility is particularly useful
because the user will be put into the local debugger between each
instruction and is therefore able to inspect the program state.

Around this time the assembler was also extended to emit checks for undefined
global variables. In Larceny, all possible global variables exist in the
default global environment, but most of them are uninitialized (or, rather,
initialized to the magic value \verb+#!unspecified+). A simple check at
the time of reference of a global variable made it much easier to find
some bugs. This facility should have been implemented earlier.

\subsection{Generic Arithmetic}

With the system in a fairly coherent state, we turned to getting the generic
arithmetic to work. The basics of generic arithmetic had been with us all
the time, whatwith checking fixnum tags and overflows and so on, but 
only fixnum arithmetic had been properly debugged so far.

There are six representations for numbers in Larceny: fixnums (small exact
integers), bignums (large exact integers), ratnums (exact rational numbers),
flonums (inexact rational numbers), compnums (inexact complex numbers), and
rectnums (exact complex numbers). When a Scheme program calls the {\tt +}
procedure, the operands are checked to see if they are fixnums. If so, the
addition is performed in-line, and if the result did not overflow, we're
done.  If the operands are not fixnums or the result did overflow, the
operands are passed to an assembly-language addition subroutine. This
subroutine does a dispatch on the types of the operands. If the types are
the same, then a type-specific operation is invoked; otherwise, both
operands and the operation are passed to a procedure known as {\tt
contagion} which does the necessary type conversion and retries the
operation.

Some of the type-specific operations are done in the assembly code: flonum and
compnum operations, and fixnum operations leading to bignum results.
Everything else, including {\tt contagion}, is delegated out of the assembly
code to Scheme support routines. In order to get generic arithmetic to work
we therefore had to design and implement calling conventions for calling
Scheme from assembly code. The problem here is that a Scheme procedure which
calls assembly code to get an operation done (e.g. {\tt +}) is not required
to save its state. However, since Larceny uses a caller-saves dicipline, the
state of the original caller must be saved when assembly code calls out
to a Scheme support procedure, and the state must be restored when the
support procedure returns, yet the original caller will not restore it.

We designed a general calling convention which performs the necessary saving
and restoring of state and which is also re-entrant (an absolute
requirement).  We could now test the bignum procedures, which are the most
interesting generic arithmetic procedures written in Scheme.

The bignum code was written a long time before it was actually tested in
Larceny, and early tests were done under Chez Scheme. However, many problems
with the bignum implementations were hidden by the test system. For example,
a bignum procedure would invoke the generic {\tt +} operation on two fixnums
and get an overflow, which eventually would bring the system back into the
bignum code again. In Chez Scheme this went undetected because the native
bignum procedures took care of the overflow; in Larceny, we ended up in an
infinite loop when a fixpoint was reached. This problem was actually
anticipated for addition, subtraction, and multiplication by doing in
assembly code all operations on fixnums which lead to bignum results.
However, some problems persisted for a long time, and at present, bignum
division is still not working correctly. A major problem is predicting
exactly which operations may cause bignum results and deal with them as
special cases.

\subsection{The Evaluator}

At some point during the spring (after about one year of work), generic
arithmetic was temporarily abandoned and a top-level loop and evaluator were
implemented in order to facilitate interactive debugging and programming.  A
facility for loading compiled files into the running system was implemented.
This was a major win in terms of development efficiency, as the creation of
a full heap is a rather slow operation, usually 5-10 minutes per heap
creation.

After the evaluator was working, programs could be compiled with the Chez
Scheme development system and then could be loaded into Larceny for
interactive testing and benchmarking. In particular, we compiled Twobit (the
Larceny compiler) itself and loaded it into Larceny, and we can now compile
(but not assemble) Scheme programs within Larceny itself.

\subsection{Exception Handling}

As we were running programs interactively (and getting bugs), it was clear
that the exception handling in the current system was totally inadequate.
We therefore designed and implemented a more comprehensive exception system,
allowing us to run buggy programs interactively without crashing the system.
This work is still not complete. In retrospect it seems clear that debugging
could have been much more swift had a fine-grained exception system been
implemented earlier on.

\subsection{Present State}

As it stands, Larceny is 75\%\ operational. There are lots of bugs, to be
sure, and some functionality is missing, particularly in terms of
mathematical functions and generic arithmetic. However, with the compiler
running, we are almost self-sustaining, and as soon as the bignum code
works, the assembler and many of the rest of the missing parts will also
run, at which point we can in principle move the development system into
Larceny.  I believe that the next version of the compiler can and should be
ported to run under Larceny rather than under Chez Scheme. Certainly,
there's a long way to go to a stable, flexible system, but the current
system is good enough for our research purposes, and it also has good
performance.

\section{Tools of the Trade}

In this section we briefly present some of the standard debugging and
development tools used in the bootstrapping and debugging process, and
comment on their usefulness.

\begin{itemize}
\item
{\tt adb} is the UNIX assembly language debugger. It is very low level and
has an extremely cryptic command language, but it has
 the redeeming feature that
the command language is semantically close to the machine level, making
debugging rather convenient. {\tt adb} has a number of misfeatures and needs to
be extended to be generally useful, but for some debugging in the very early
stages and when the window of error had been narrowed sufficiently, it was
invaluable.

\item
{\tt gdb}, the Project GNU debugger, is a source-level debugger oriented 
more towards high-level languages. The current version of {\tt gdb} does
assembly language debugging fairly well, but its expression syntax is still
that of C, and when debugging at the machine level, the semantic gap between
what you want to do and what you need to tell it is sometimes too high.
Redeeming features of {\tt gdb} include an interaction mode with {\tt emacs}
and sophisticated source level debugging when that is needed.

\item
{\tt emacs}, the Project GNU editor, was a major aid in the development.
First, it provided an unobtrusive syntax-directed editing facility, making
programs easier to write in several languages. Second, by running programs
from within {\tt emacs}, the output of the program can be captured in an
{\tt emacs} buffer, and it is possible to scroll back and inspect the
execution which has ``gone off the top of the screen''. This facility is
particularly important when stepping through long sequences of machine
instructions, be it in {\tt gdb}, {\tt adb}, or the local debugger.

\item
{\tt RCS}, the revision control system, was used to keep backups and to
control versions. Lately, with several people now working on Larceny, it
provides access control to the source files as well. We also wrote some
tools on top of RCS to make it easier to manage the large number of files
we have as a unit.

\item
{\tt Make}, the UNIX build utility, was used for building UNIX executables.
For building a heap, however, we implemented our own make utility in Scheme.
This is a simpler version of its namesake, but it provides the necessary
timestamp checking, and it saves us a great deal of time and effort.

\item
{\tt config} is a program we wrote to simplify the task of keeping track of
constants in several languages. There are a lot of constants in Larceny:
indices into tables, register numbers and names, names of operations, and so
on. For a while, these constants were maintained manually in header files
customized for each language (C, assembly, and Scheme). However, as the
system evolved and the number of constants got larger, bugs started
to appear when the constants got out of sync. As a result, {\tt config} was
written. Its input is a language-independent description of constants; its
output is several header files, one for each language. A customization
feature allows an arbitrary Scheme procedure to be applied to the constant
for a particular language to create a custom constant.

\item
Documentation is of course essential for a large project. From very early
on, documentation was done in terms of a (growing) set of ``Larceny Notes''.
Each note documents one or a small number of aspects of the design or
implementation of the system. It is easy to change a note, it is easy to
write a new one, it is easy to find one, and it is easy to read one. In sum,
it is easy to create and maintain documentation. In retrospect it may have
been better to use the Project GNU {\tt texinfo} package rather than \LaTeX,
since {\tt texinfo} can generate online documentation as well as paper
copies.

\end{itemize}

\section{The Memory Management Subsystem}

Like I mentioned earlier, the garbage collector was hand-checked extensively
before being actually run on any data. At the time the collector was being
tested, however, there were no heaps to test it on, nor were any forthcoming
anytime soon. The solution was of course to create dummy heaps which we knew
what would look like and which we could run the garbage collector on,
knowing what the resulting heaps would look like. These heaps would be very
small, no more than 20 or 30 words or so, and would be described by hand.

A fairly elaborate scaffolding program was written which would take as its
input heaps described in a heap description language followed by heap
operations (change this cell, perform that kind of collection).  The program
would create the heap as per its instructions, and then execute the
commands. One particular command would print the heap state on the standard
output for the operator to inspect; this lent a bit of flexibility to the
process because
intermediate states could be inspected easily. Figure \ref{heapfig} shows
an example of a small test heap.

\begin{figure}[ht]
\begin{verbatim}
  ; Test #2 -- ephemeral collection with entry pointers.
  ; We have two structures in the e-area, one pointed to by a root, 
  ; one pointed to by a structure in t-space, a pointer to which 
  ; exists in the entry list. The structure in the tenured space is a pair.

  ROOTS
  E 00000001      ; root 0: pointer to pair at loc 0
  EPHEMERAL
  C 12345678      ; loc 0: (car) fixnum
  C 0000000a      ; loc 4: (cdr) nil
  C aaaaaaa8      ; loc 8: (car) fixnum
  C 0000000a      ; loc 12: (cdr) nil
  TENURED
  E 00000009      ; loc 0: pair pointer into ephemeral space loc 8
  C 0000000a      ; loc 4: nil
  ENTRYLIST
  T 00000001      ; pair pointer into tenured space loc 0
  E-COLLECT
  POINTERS
  R-DUMP
  E-DUMP
  T-DUMP
  Y-DUMP
  END
\end{verbatim}
\caption{An example scaffolding heap}
\label{heapfig}
\end{figure}

The heap in figure \ref{heapfig} has two main areas, known as the
``Ephemeral'' and ``Tenured'' areas. It also has an area containing pointers
to objects in the tenured area which contain pointers into the ephemeral
area; this latter area is known as the ``entry list'' (later it changed name
to the ``transaction area'') and its purpose is to keep track of all
intergenerational references from the tenured generation to the ephemeral
generation. There is also a set of ``roots'', from which the garbage
collector will start its traversal. Data entries in the heap areas are of
three types: ``C(onstant)'', ``E(phemeral pointer)'', or ``T(enured
pointer)''.

At one point, the language was extended to allow some dynamic behavior, like
assignments into data objects (e.g. {\tt set-car!}) and memory allocation
(e.g. {\tt cons}). This dynamic sublanguage let us test some of the dynamic
parts of the system when they became available, increasing our confidence in
them.

After having created a lot of different heaps with interesting configurations,
the collector was declared to be trustworthy. As we saw above, however,
bugs did remain. On the whole, though, this scaffolding was very successful.

A source of bugs related to the collector are memory area overruns. For
example, the stack cache is always flushed before a garbage collection.  If
we are not careful, there will not be room in the ephemeral area to flush
the cache in. In our case, most cases were thought out beforehand, and this
was not a major problem.  Still, overflow areas are suspect at best, and in
some cases they have overflowed (!) without detection. Designing without
overflow areas is a definite advantage.

Some bugs in the stack cache handling were caught by tests in the stack
cache routines which checked the stack pointer and the return address in the
stack frame for validity, and which would flag an error if a questionable
value was found.

Another problem area is the boundary cases: what happens when we need to
allocate memory when we are running in an assembly language subroutine which
violates the Scheme virtual machine in subtle ways? The simple answer is
that we may not allocate memory while in such a violated mode, but sometimes
we need to. We can design special-purpose allocators to deal with these
cases. The problem is that it is sometimes difficult to know (and remember)
when we are violating the virtual machine invariants. 

It is vital to test the garbage collector as fully as possible as early
as possible, while other parts of the system are not yet playing a role.
Bugs in the collector often manifest themselves a long time after they
occured and can be nearly impossible to track them down.


\section{The Assembler}

Implementing the assembler was interesting in several ways. First, Scheme is
not the ideal language for implementing an assembler: the bit-twiddling
operations needed to create the machine code have to be synthesised from
arithmetic operations. Second, getting the bit-patterns right is always
an interesting problem; read the manual wrong and you have a bug which can
be very difficult to find. Third, I had never written an assembler before,
and one could argue that the finished program looks correspondingly.

The MacScheme-Assembly-Language to SPARC-Assembly-Language code generator
in the assembler has implicitly encoded in it much knowledge about the SPARC
architecture and its restrictions and features, like the size restriction on
the immediate field in an instruction. If the code generator is not careful
when it generates SPARC code, it may end up generating instructions which
have been corrupted by an overflow of the immediate field. A particularly
interesting variation on this occured in very rare cases when the generated
code vectors got very long (over 1K instructions). The immediate field in a
SPARC instruction is in general 13 bits, signed, restricting positive offsets
to 12 bits or 4K, i.e.  1K instructions if we are talking about distances to
a label from the current program point. Some MacScheme instructions require
the calculation of the absolute address of a label. One standard idiom for
this on the SPARC is this:

\begin{verbatim}
    call    .+8                   ; get address of this instruction
    add     %o7, (L1-(.-4)), %o7  ; calculate address of label L1
\end{verbatim}

This only works, however, if the label L1 is within 1K instructions from the
{\tt call} instruction. When procedures get large (as they sometimes do,
especially in Scheme when full in-line typechecking is turned on, as it
should be), this instruction will fail to calculate the right address, and
what you have is a program which suddenly falls off the deep end when it
tries to jump to the calculated address. Often, you find these bugs only
by single-stepping through massive amounts of instructions in a low-level
debugger. Sometimes you can prune the search space by looking for features
of the offending Scheme code which have not been tested before, and go from
there. Ideally, your assembler should tell you about these things; the
second major design mistake in the assembler was to make it so trusting of
its input.\footnote{The first major design mistake was to rely so heavily on
bignums for an intermediate representation.} A disassembler can be
invaluable, but the number of instructions to hand-check can be prohibitive
(and disassemblers have bugs, too).

The assembler was also plagued with typing (i.e. keystroke) mistakes. One
could make an argument that these mistakes would have been caught in a
statically typed language; they might also have been easier to notice if I
had avoided using the quasiquotation notation in Scheme, as many of the
typing errors were a forgotten unquotation.

The main problem with the assembler was that it was not tested before being
put to use. Not testing it properly was an evil thing to do, especially in
light of the number of bugs it had.

\section{Generic Arithmetic}

Testing the generic arithmetic system is {\em hard}. There are a number of
representations (six, in Larceny) and therefore in general thirty-six
combinations of arguments for each binary operator (although not all
operators can take all types as arguments). In addition, the IEEE Scheme
standard specifies behavior related to the exactness and the inexactness
of numbers, and how inexactness is a contagious attribute.

In order to test the generic arithmetic, therefore, I decided on using a
regression testing technique. Properties of the arithmetic system would be
tested in batches, where each batch would be allowed to rely on the
established correctness of operations tested in a previous batch. So, for
example, one batch would test arithmetic on fixnum operands resulting in a
bignum result; the next batch could then rely on these operations to test
bignums more fully. Often, a batch of tests was characterized by the
boundary cases it tested. 

Some test scaffolding was written: the testing procedure would take a
printable representation of the expression under test and two additional
arguments. The first of these was the result of evaluating the expression
under test. The second was the correct result. If, according to {\tt
equal?}, the two values were different, the test was defined to have failed
and a diagnostic (containing the printable representation of the expression)
was printed, and when the batch had finished, testing would terminate. The
programmer had to go back and fix all bugs in the given batch, and when all
passed, the tester would proceed to the next batch.

It is to be noted about this testing methodology that a fairly intimate
knowledge of the implementation is required in order to know where the
boundary cases are. While one could make an argument that the specification
of the basic representations should be enough, this is true only to an
extent, because the specification does not always lead one to consider all
the special cases that the implementation would have to consider and which
in many cases are a result of the underlying architecture.

This form of regression testing has been successful in finding many bugs
in the generic arithmetic system. Among its chief advantages is that it
makes it easy to do proper testing (because once the test suite is written,
it can be rerun automatically), and it also makes it easier to think about
the test cases and to organize them logically and in increasing order of
difficulty.

What we should have done in Larceny, but did not, was to use a macro
processor like UNIX' {\tt m4} for some of the assembly language programming.
There is much duplicated or nearly-duplicated code in the generic arithmetic
assembly code, with only small variations. A macro processor would have let
us factor these out and in turn maintenance would be easier.


\section{Concluding Remarks}

In this section I will attempt to summarize the paper and also present some
insights about debugging systems of the same type as Larceny, insights I
will pay attention to when it comes time to port Larceny to a different 
architecture.

After 15 months of mostly part-time work, Larceny is finally operational and
reasonably stable. How did we get here, and what could we have done
differently? I think that we did a lot of things right, especially in the
order in which pieces were designed and implemented.\footnote{Much of the
glory for this particular point goes to Will, who had done it all before and
who could guide me.} A definite focus on simplicity also helped. As for
the debugging, if I were to do it again, I would make the design simpler
(see below) and I would be more careful about some of the implementation
decisions which seemed good at the time, but turned out to be problems,
like the calling conventions in the assembly code. I would also test my
tools better; the module testing of the assembler was on the border of
the criminally negligent.

Some points to ponder are:

\begin{itemize}

\item
{\em Consider good software engineering practices.} The development and use
of the {\tt config} program and other tools was a definite advantage, as was
the scaffolding of the garbage collector and the regression testing of the
generic arithmetic. All of these are well-known and often-taught practices.
Use tools like {\tt m4}.

\item
{\em Avoid system invariants.} The fewer invariants you have (you will need
some), the fewer there are to violate. In particular, avoid overflow areas
like the plague if you can, as they usually cause problems by being too
small or too big. Larceny has several of these areas; in retrospect, they
could all have been avoided, and they will be removed in a future version.

\item
{\em Do it cleanly.} Restrictive calling conventions are your enemy. Make
them general and flexible, even if it comes at a cost of some performance.
Also, count on a space crunch in some cases and decide on a proper strategy
for allocating temporary variables when you need them.

\item
{\em Do it defensively.} Design early for the debugging which will come
later.  In particular, design and implement at least a rudimentary
fine-grained exception handling system, even if it will later have to be
rewritten. A coarse-grained system will not give you enough information
about errors in your code.

When you design software tools, make them at least a little robust.
Even if their input is supposed to be bug-free, the generating agent (a human
or another program) sometimes makes mistakes. Catching these mistakes early
on is important.

Make your system diagnose itself. While the local debugger was only a
limited success (it wasn't quite powerful enough), there are other options.
Programs like {\tt gdb} can call procedures in the program being debugged,
so you have the option of inserting debugging code which is called from
nowhere in your program, but which can be called on demand from a debugger.
Insert sanity checks and assertions where reasonable.

The single stepper should definitely have been implemented earlier.

\item
{\em Test your tools before use.} This should be obvious, but I'll say it
anyway. Yes, it is a pain to test all instructions in the assembler.  It is
a pain to test all cases in the disassembler and check the output.  Do it
anyway.

\end{itemize}

\end{document}
