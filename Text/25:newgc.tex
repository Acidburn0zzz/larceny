\documentstyle{article}
\newcommand{\reg}[1]{{\sc \%#1}}

\topmargin      -2.0cm
\oddsidemargin   0.0cm
\evensidemargin  0.0cm
\textwidth      6.5in
\textheight     9.5in
\parindent       0.0cm
\parskip         0.4cm

\title{Larceny note \#25: Larceny Mark II}
\author{Lars Thomas Hansen}
\date{July 16, 1994}

\begin{document}
\maketitle

\abstract{

Versions of Larceny through 0.19 suffered from a number of
implementation-related problems. These problems were in great part due to
the first-system nature of Larceny, and it was long clear that if Larceny
were to be stable and maintainable, it would have to be rewritten.
Especially troubling was the convoluted nature of the garbage collector, the
poor performance of the stack cache, and the large amount of assembly code
in the system.

As part of the effort to write a paper (based on my thesis) for which I need
a system with fewer rough spots than the original, I have performed an
almost complete rewrite of Larceny; all C and assembly code has been cleaned
up and for the most part rewritten extensively. The resulting system,
Larceny v0.20, is much cleaner, significantly easier to understand, more
modular, and has better performance, than v0.19.

This paper describes the problems of the old system and the implementation
of the new system.
}

\section{Introduction}

In its first incarnation (versions through 0.19), Larceny suffered from
several problems. These problems were in great part due to the first-system
nature of Larceny: I had never written such a program before, and had to
learn by doing. In summary, the problems were these:

\begin{itemize}
\item
The garbage collector was too complicated. It knew too much about the layout
of the heap and the stack and spent time worrying about these and about what
might or might not overflow; it should instead have been spending its time
copying data around.

\item
The heap layout was too complicated. Since the stack cache would have to be
copied into the heap on overflow, the collector set aside an overflow area
for the stack to be copied into in case the heap was full. In addition, the
stack cache itself could overflow, so the code contained obscure
calculations of the sizes of these overflow areas, and several different
kinds of heap limits and pointers existed to keep track of it all.

In addition, there were two documented special cases which allowed the heap
and stack pointer to be invalid on entry to the garbage collector, on the
assumption that this would make it easier to generate good code. These cases
have been outlawed; the heap and stack pointers in the new system must be
valid at all times.  The performance impact of this constraint is expected
to be unnoticeable.%
\footnote{The moral of the story, of course, is that premature optimization
is evil, and that premature pessimization is doubly evil.}

\item 
There was too much assembly code in the garbage collector. The file {\tt
memory.s} which provided hooks for call-outs from Scheme code to the memory
management system contained over 1100 lines of code. Maintaining and
understanding so much code is hard. Much of the code was again due to a
misplaced sense of what ought to be ``optimized''; the rest was due to
dealing with the special cases in the collector.\footnote{The combination of
``special cases'' and ``optimal'' is rather deadly.}

\item
There was too much assembly code in general, scattered in several different
files, with little control over what which files ``knew'' about the run-time
system and the system tables.

\item
The remembered set implementation was too simple: in practice it was prone
to page faulting in tight memory because it could grow without bound and
with poor (sequential) locality; not only did updating the remembered set
cause page faults but the remembered set would have to be scanned twice 
during a garbage collection.

In addition, the code which entered transactions into the remembered set was
not particularly discerning: as long as the left-hand side of the assignment
was a tenured object, it was remembered, regardless of the right-hand side.
Together with the ever-growing remembered set, this caused poor performance
in certain programs.

\item
The stack cache implementation suffered from slow overflow logic: on an
overflow the cache was copied, frame-by-frame, into the heap. For reasons
which are not entirely clear the copying code (which was written in C)
tended to be quite slow, possibly due to too much protocol.

\item
In general there was too much global knowledge about the layout of the system
data structures: the run-time system was small (7000 lines) but all the files
``knew'' everything. Any change had potentially global effects.

\item
The context-switching code (going from C to Scheme, from Scheme to C, and
from Scheme to Scheme via millicode) was somewhat ad-hoc and determined
on a case-by-case basis, leading to problems with quirky calling conventions
in the assembly code.

\end{itemize}

The new run-time system solves all these problems, more or less. The net
result of the modifications is a system which is much simpler to understand,
is more flexible, is more modular, is better documented, contains less code,
and has better performance, than the old system. The basis for the system is
version 0.19 (from here on known as ``the old system''), but with the
exception of the millicode for generic arithmetic, all C and assembly source
files have been changed extensively.  The new system has version number 0.20
(and is referred to as ``the new system'' in the following).

\section{Containment and Information Hiding}

In the new system, knowledge of the system's structure is distributed but in
general private. The garbage collector has complete control over the layout
of the heap, but the structure of the heap is not general knowledge; the
garbage collector exports a few variables and procedures with known
properties, and the other parts must relate exclusively to these.

Similarly, the remembered set implementation is abstract; the garbage
collector simply calls a procedure \verb+enumerate_remset()+, passing it a
callback procedure which is called once for every remembered object.

The C language files have been split into more, smaller, files, each with
clearly defined responsibilities.

The assembly language files have been in large part rewritten and each
of the few files have clearly defined responsibilities (outlined later).
The amount of assembly language code has gone down from 4600 to 3700 lines;
$2/3$ of what is remaining is in generic arithmetic, which has not in general
been affected by the rewrite. The amount of C code has remained roughly
constant.

The new system is layered in the following way; the arrows indicates the
direction of a procedure call:

\begin{verbatim}
                    |
              +---- V -----+
              |            |
              |   Scheme   |
              | Entrypoint | +------------------+
              | (assembly) | |    Runtime (C)   |
              |            | |                  |
              +---- | -----+ +------- ^ --------+
                    |                 |
              +---- V -----+ +------- | --------+
              |            | |                  |
              |  Compiled  | |    C callouts    |
              |   Scheme   | | (C_* and UNIX_*) |
              |            | |                  |
              +--- | ^ ----+ +------- ^ --------+
                   | |                |
              +--- V | -------------- | --------+
              |                                 | +------------------------+
              |  Millicode (assembly language)  -->  C runtime arithmetic  |
              +---------------------------------+ +------------------------+
\end{verbatim}

By structuring the system according to this picture, the points of context
switching have become clear, and a more uniform model for what modes the
virtual machine might be in and for how it should switch contexts has been
worked out.

\section{Memory Management}

\subsection{General}

The files which make up the memory manager are these:

\begin{description}

\item{{\tt Sys/policy.c}} deals with heap initialization, garbage collection 
policy, and similar high-level issues. It is the only file which contains
detailed knowledge of the heap layout.

\item{{\tt Sys/gc.c}} is the low-level part of the collector. Its only task 
is to perform a collection of the requested type as quickly as possible.
This file is small, a scant 200 lines including comments. If performance
needs dictates, it would be feasible to code this in assembly language.

\item{{\tt Sys/remset.c}} is the implementation of the remembered set.

\item{{\tt Sys/stack.c}} is the implementation of the stack cache.

\item{{\tt Sparc/memory.s}} contains entry points into the garbage collector, 
callable from Scheme or from millicode. The number of entry points have been
reduced drastically (more work is now done in-line), and the size of the
file is down from 1100 lines to 350 lines. The code in this file makes 
very few assumptions about the heap layout.

\end{description}

The new garbage collector is able to expand the tenured heap when it is
full. Since I wanted this ability (it makes programs perform better because
they will no longer thrash in a tight tenured space) but did not really care
about co-existing with C code, I wrote a drop-in replacement for {\tt
malloc()} and friends; this replacement (in the file {\tt Sys/malloc.c})
allocates from a fixed-size arena and does not upset Scheme memory
management. The new garbage collector manages an expandable heap using the
{\tt sbrk()} system call. Sometimes the heap is also contracted; this helps
control heap growth, as explained below.

The limits of all heap areas are kept in the {\tt globals[]} table; the
following naming scheme is used. The first word of a heap area {\tt X} is
pointed to by a global named \verb+G_X_BOT+. The first word past the heap
area is pointed to by a global named \verb+G_X_LIM+. The first free word in
the heap area is pointed to by a global named \verb+G_X_TOP+. When {\tt TOP}
is equal to {\tt LIM}, the heap area is full.

Since we have low tagging, the use of a limit which points beyond the top of
the heap makes it easy for the garbage collector: a tagged pointer can be
compared to a heap limit without stripping the tag off the pointer or
inserting a tag in the heap limit before the comparison. This works because
all allocation is in 8-byte increments and we have 3-bit tags; the highest
pointer into a heap area with tag 7 can never be equal to the heap limit,
because stripped of its tag it must point no higher than the heap limit
minus eight bytes.

\subsection{Allocation}

Allocation in the new collector is like in the old: there is a register,
\reg{ETOP}, which points to the next available word. There is another
register, \reg{ELIM}, which points to the first word above the available
heap. When \reg{ETOP} moves past \reg{ELIM}, the heap is full. There is an
out-of-line procedure which can be called to perform allocation, but
allocation can validly be done in-line, and the code for {\tt cons} does
this. A complete {\tt cons}, including heap allocation, initializing the
cell, and tagging the pointer, takes 6 instructions on the Sparc.

The old system had a documented special case: it was legal for allocation
code to leave the heap top pointer pointing above allocatable space. This
was thought to be a benefit to the allocator. In practice, it only causes
grief for the collector, and that special case has been done away with.

\subsection{Collection}

This section describes the lower half of the collector, contained in the
file {\tt Sys/gc.c}.

The old garbage collector had a heap overflow check in the heavily used {\tt
forward()} procedure. This check was unnecessary during ephemeral
collections, it was on the speed-critical path of the garbage collector, and
it probably confused the C compiler because it contained a procedure call
which would be executed if the heap overflowed. The new collector does not
have the overflow problem since the heap can be expanded.  The details are
explained along with the explanation of garbage collector policy, below.

The new collector is very small, and has two entry points: 
\verb+major_collection()+ and \verb+minor_collection()+. The former performs
a full garbage collection from one tenured space and one ephemeral space to
the other ephemeral space. The latter performs the ephemeral and tenuring
collections where an ephemeral space is copied to somewhere else
(respectively to the other ephemeral space and to the current tenured
space).  Both these procedures take three parameters: the limits of
fromspace, and a pointer to the free area of tospace. 

The collector routines know nothing about the structure of the root set.
They call on two external procedures, \verb+enumerate_roots()+ and
\verb+enumerate_remset()+, passing them a callback procedure which will
forward the root or scan the tenured object as needed. Since C does not
have closures, some more parameters are passed as well, but these are
simply passed on by the enumerator procedure to the scanner procedure.
Since the collector is oblivious to the structure of the remembered set
and the roots, these structures can be changed without affecting the
collector. If the stack were to be scanned rather than spilled at the
time of a collection, a similar method should be implemented for it.

The collector routines don't know anything about the layout of the heap,
either! As far as this part of the collector is concerned, the heap is
limitless and can be scribbled on anywhere (although sequentially). All that
matters is to shuffle data as quickly as possible from one area to the
other.

\subsection{Garbage Collection Policy}

This section describes the upper half of the collector, contained in the
file {\tt Sys/policy.c}.

The two main entry points here are the procedures \verb+allocate_heap()+,
which sets up the heap areas, and \verb+garbage_collect()+, which performs
a garbage collection. 

\verb+Allocate_heap()+ takes a number of arguments pertaining to
the requested sizes of the areas and to garbage collection policy. It
allocates the heap and initializes a number of private heap pointers. It
then creates a stack by calling on the \verb+create_stack()+ procedure
(defined in {\tt Sys/stack.c}) and returns. It does not deal with the
remembered set at all and has no knowledge of the existence or structure
of this set. (This is not necessarily a feature, since the garbage
collector procedure has to deal with the remembered set anyway, so it's
not like it's hidden from the procedures in this file.)

\verb+Garbage_collect()+ takes two arguments: the requested type of
collection, and the minimum number of heap words to free up. Based on
the status of the heap areas, state variables, and so on, the collector
decides on the kind of collection to be carried out. It then calls the lower
level of the collector to perform the collection.

When the lower level collector returns, the collector again looks at the
status of the heap areas and so on. If memory is tight, it may decide to
perform another, bigger collection, or to expand the heap area. If data was
tenured, the remembered set is cleared.

Some of the policy may be worth explaining. There are two tenured spaces,
called A and B. They are of the same size, and A lives at lower addresses than
B (although this is not important). Following A there is an overflow
space the size of an ephemeral space. (If the stack were to live outside 
the ephemeral area, it would be the size of an ephemeral space plus the size
of the stack cache.) The purpose of this overflow area is to make it possible
to perform a full collection from space B to space A easily, and to perform
a tenuring collection into space A without worrying about overflow.

Consider this scenario: we must perform a tenuring or full collection from
space B to space A, but the live data in the tenured area plus the live data
in the ephemeral data may overflow A. With the overflow area, this is not a
problem. Without, we would have to somehow grow A underneath B, or perform a
full collection sooner. The overflow area makes the collector simpler. It
is the only overflow area in the system, and the calculation of its size
is straightforward.

The overflow area also makes it easier to put off a full garbage collection:
if space A is active and a tenuring collection might overflow it (because we
must assume that all of the ephemeral space is live), we can still just do
the tenuring collection and then check for overflow. If the space
overflowed, we schedule a major collection next time around; otherwise, we
don't do anything special.

In addition, the tenured aera is expanded if, after a major collection into
area A, there was more live data than the high watermark (which defaults to
75\%, but this has not yet been tuned).

There are some problems, however. The aggressive heap expansion policies
tend to cause the tenured area to grow beyond the bounds of physical memory
if the growth is not somehow stopped; the growth does not automatically stop
when there is enough memory to accomodate all live data. This is not hard
to see: we perform tenuring collections into A until it overflows, at which
point we have to expand the areas and collect into B. Data is tenured into
B until it may overflow, at which point data is copied back into A again, and
the cycle repeats. The result is an ever-growing tenured area.

Currently two mechanisms are being used to control heap growth.  First, we
contract the tenured area if, after a major collection into area A, not
``much'' data is live; the value of ``much'' is determined by the low watermark
(currently at 50\%, also not tuned). Second, if a tenuring collection is
about to be done into area A and the expected amount of live data (currently
set at $1/3$ of the ephemeral area size) would overflow the tenured area,
a full collection is performed instead, without any heap expansion.

The measurable effect of the growth control is to increase the CPU time of
the program (because garbage collection costs go up when memory is tighter);
however, the CPU time matches the elapsed time of the program much more
closely than when running without growth control. We lose on CPU time
benchmarks but win in practice.

\section{The remembered set}
\label{remset}

The old remembered set implementation recorded transactions in a contiguous
region growing from the high end of the current tenured space toward lower
addresses. Any assignment which had a tenured object on the lhs caused a
transaction to be recorded, so for certain programs the rate of transaction
recording was quite high (and much higher than necessary). The argument for
recording so aggressively was that it was simple (i.e. faster) and that
probably, most assignments to tenured objects would create intergenerational
pointers. A techical report by Benjamin Zorn argues otherwise
\cite{zorn-tr}; his estimate is that a very low percentage of the assignments
create intergenerational pointers.

When the transaction area filled up in the old collector, there were two
possible courses of action: try to compact it, or do a full collection.  (A
tenuring collection was not possible since the transaction area was
occupying the space; in retrospect the transaction area could at least have
lived in the tenured semispace not in use.)  Compaction can be performed
either by an ephemeral collection or isolated; the old system performed an
ephemeral collection, increasing the rate of collections and disturbing data
unnecessarily.  In addition, if memory was tight, the transaction recording
scheme caused many page faults, first during recording and then during gc
(which has to make two passes over the transaction area during an ephemeral
collection). Compaction was done by setting a bit in the header of a tenured
object during scanning of the remembered set; the bit would then be turned
off when gc was complete.

The new remembered set implementation attempts to avoid all these problems.
It is implemented in two parts: a static store buffer (SSB) which is an area
in which transactions is recorded; and a hash table which is used to 
remove duplicates.

The SSB size defaults to 64 KB (16K transactions); whenever it is full, its
contents are entered into the hash table and it is cleared. Only assignments
which have a tenured object on the lhs and an ephemeral object on the rhs
cause a transaction to be recorded in the SSB. There is one entry point
in {\tt Sparc/memory.s} to check and record a transaction; this entry
point receives the lhs and the rhs and records a transaction if necessary,
triggering a compaction of the SSB if it becomes full. The assignment
itself must be performed by in-line code {\em before} the transaction is
recorded. All code except for transaction recording is written in C.

If the hash table fills up during a compaction of the SSB, an ephemeral
collection is performed. If the hash table remains relatively full after a
collection, a tenuring collection is scheduled. [Not implemented.] The hash
table is a power of two in size and contains pointers into a separate pool
for the remembered transactions.  The default size of the table is also 64KB
(16K buckets). The hash function is simple: after stripping the tag and one
more bit, enough bits are extracted to make a hash table index. For a 64KB
table, for example, bits 4\ldots 18 are used. The table chains on
collisions.

The pool of transaction consists of two-word entries; it can be as large as
we want and can grow if we like it to, but the current implementation has a
fixed-size buffer defaulting to 64KB (8K entries). Each entry consists of a
tagged pointer and a pointer to the next word in the chain.

This scheme was inspired by a paper by Hosking, Moss, and Stefanovic in the
OOPSLA '92 proceedings \cite{remset-paper}. Their hash table is non-chaining
with limited searching on collisions and resizing of the table if the search
does not find a free slot. The two main advantages of my scheme over theirs
is that a) mine is simpler, and b) the time it takes to scan transactions
during a minor gc is proportinal to the number of transactions with my
scheme, while their scheme needs time proportional to the size of the hash
table.

The main disadvantage of a hash table for the remembered set is that the
table (although not the SSB or the pool) needs to be zero-filled when it is
cleared (during a tenuring or full collection). If the hash table is large,
this can take significant time compared to the time it takes to perform a
tenuring collection. However, I do not expect this to become a problem.

\section{The stack cache}

In the old implementation, the stack cache was kept in a separate,
fixed-size area. On a garbage collection, on a continuation capture, or on
an overflow during a deep recursion it would be copied, frame-by-frame, into
the ephemeral heap; each stack frame would become a vector in the heap and
the vectors would form a chain of frames isomorphic to the dynamic chain of
calls.  In practice, a stack flushing operation was quite slow. The reasons
for this performance problem are not entirely known. Part of it was probably
that the frames in the stack were slightly different from the frames in the
heap (in particular the stack frames were a word smaller) causing the
copying code to work harder on converting: the frames had to be copied one
at a time.  Part of it was possibly the poor performance of the Sparc's
memory subsystem.

The new implementation does not have a separate stack cache area. (I hasten
to add that a separate stack cache area can be added at very little cost or
increase in complexity; there are advantages to having a separate cache. See
below.) Instead, the stack cache lives in the ephemeral space: it grows from
higher addresses toward lower addresses. Each stack frame is the same size as
and has the same layout as the vector it may at some time become, although not
all fields are initialized. Taken together, these two changes make it
possible to flush the stack in-place, and they also make it possible for the
stack to grow to fill the available space in the ephemeral heap. In
addition, we save two registers: the stack pointer is now the heap limit,
and the heap top pointer is the stack limit, so separate heap and stack
limit registers are no longer needed.

The advantages of the new scheme are fast flushing and more flexible growth
possibilities. However, sometimes it is desirable to have the stack in a
separate area. If the stack is in the heap, it cannot simply be scanned on a
garbage collection: it must be flushed. Also, keeping the stack in the heap
makes it impossible to implement multiple threads with separate stacks; we
must use {\tt call/cc}.

If the stack is kept separately, some reasonable guidelines are these. The
format should be the same in the stack and the heap so that it is quick to
make a traversal of the copied frames. The stack cache can then be
block-copied into the heap using some presumably optimal system routine%
\footnote{On Unix systems the primitive {\tt bcopy()} is usually very highly
tuned, since ``to the first order, the kernel is {\tt bcopy()}'': the kernel
spends most of its time copying data around.} and subsequently walked with a
fast stack walker like the one in the current system. In fact, if the stack
is flushed as part of a garbage collection, it should be copied directly
into the destination area, since it is known to be live. This may in fact be
{\em faster} than flushing it in-place and letting the collector copy the
frames\ldots The old system always copied it into the current heap area.

The method for stack allocation has changed. In the old system, the program
would compare the stack pointer to the stack limit, and if the comparison
was favorable (the pointer was above the limit) the stack frame could be
allocated, regardless of size; there was an overflow area to take care of
``any'' frame size. This has been changed; the current system allocates the
frame, checks the limit, and if the limit has been exceeded, the stack
pointer is restored and the overflow handler is invoked. It is one less
special case to deal with. (And anyway, the old system would not have worked
with the new stack.)

The stack underflow logic in the new system is slightly slower than in the
old system and slower than it has to be: a full context switch into C is
taken on each underflow. Underflow in the current system is complicated by
the fact that an underflow may cause a garbage collection, as there is not
necessarily room for the copy of the frame on the stack in the ephemeral
area.  Since dealing with all this in assembly language seemed excessive (I
have learned my lesson well), an underflow causes a call into C. Since a
garbage collection may occur while in C, the entire context must be saved.
The performance of the underflow logic does not appear to be a bottleneck.
However, since the stack frames are so similar to frames in the heap, it
would be easy to duplicate some of the code in assembly language if
performance is a problem, and only go to C if the heap overflows.

\section{Context switching and internal calling conventions}

In the old system, context switching was a little ad-hoc. When going from
millicode to C code which was sure not to garbage collect, the millicode
would perform only a {\tt save} instruction to get a C context, since the C
code was known not to need the virtual machine registers. On return, the
millicode would perform a {\tt restore}. This was fast, but led to knowledge
of C calling conventions and assumptions about the C procedures being
scattered about in the assembly code.

When the C code could possibly perform a garbage collection, the millicode
would have to perform a full context switch by calling on the millicode
procedure \verb+mem_save_context+; later, an inverse procedure would restore
the context again. After the context had been saved the virtual machine
would be in ``C mode'', with different register conventions and so on.
However, while this is clean, there was no clean way to save and restore the
return address to Scheme code; several ``temporaries'' existed for the
purpose. Sometimes a stack frame would be created in order to save the
return address in an orderly manner, other times, when it was known that no
garbage collection was needed, it would be tucked away in a variable. It was
the proverbial mess.

In addition, the calling conventions in millicode were poor. Since the Sparc
does not have a stack onto which it is convenient to push e.g. return
addresses, millicode calling conventions were ``designed'' to deal with
call chains which were only two calls deep; return addresses could be saved
in temporary registers. The callees were constrained in what registers they
could use.

The new system almost does entirely away with the distinction between a
quick call and a full call, and abstracts away call-outs to C almost
completely. All call-outs to C are performed by two millicode procedures
\verb+callout_to_C+ and \verb+internal_callout_to_C+ (they differ in how
they treat the return address). In addition, the Scheme return address is
always saved in one place (in the global \verb+G_RETADDR+) and is converted
properly on a context switch in case of a GC.  The only code which still
uses the old ``fast'' calls is the generic arithmetic, which needs to access
the library procedures for multiplication and division.

Millicode calling conventions have been cleaned up; the procedure which
receives the call from Scheme code must save the return address in the
global if it calls on an internal millicode procedure, and must restore it
when needed. A small stack and procedures for pushing and popping have been
provided, removing constraints on the depths of calls and availability of
registers.

Previously, when millicode needed to call Scheme code, two stack frames were
created and some magic code written in MacScheme assembly language with a
little in-line Sparc code handled the convoluted procedure return to the
original caller of the millicode. This calling sequence has been redesigned;
the called Scheme code now returns to a millicode procedure which does the
right thing.

\section{Other changes}

Some other, minor changes:

\begin{itemize}
\item
There are new command-line switches which are less ad-hoc.

\item
Filenames have been shortened to the $8+3$ monocase format to make it possible
for me to download the files to my DOS box; not all changes are complete.

\item
The globals and millicode tables have been merged, saving another register.

\item
The MacScheme {\tt apply} instruction has been changed: previously it took
an implicit argument in register 1; it now takes two explicit register 
arguments. The procedure is passed in {\tt RESULT}; the list in the first
register argument, and the length of the list in the second. The Scheme
code for the {\tt apply} procedure must both check that the argument is
a list, and calculate its length. It can do this in one pass, but currently
uses two for simplicity.

\item
The unidentified, suspected GC bugs which have been bothering us occasionally
have gone away.

\item
The manner in which memory statistics is calculated is different. Before,
statistics gathering code was scattered all over the system, peeking into
tables and looking at heap limits. Now, the various parts of the systems
call the memory statistics code with information when it is reasonable
to do so; the memstats module (in {\tt Sys/memstats.c}) is a passive
receiver.

In addition, the way Scheme accesses these variables has changed, but these
changes are localized in the file {\tt Lib/memstats.sch}.

The old procedure {\tt run-benchmark} will no longer work; however, a
compatible version is now a standard part of the heap image.

\item
A bug in the peephole optimizer has been fixed; it was not important for
any of the benchmarks in the LFP paper: it only struck when one of the
arguments to a binary predicate was in a software register.

\end{itemize}

\section{Performance}

The table at the end of the paper shows some comparative performance figures
to the old version of Larceny. For version 0.19 I used the numbers
calculated for the LFP talk; however, some of these are worse than they
should be since that collector was never properly tuned. Among other things,
I managed to shave 30\% off the running time for the slow {\tt append-1}
benchmark by changing the tenuring limit of the collector from 50\% to 25\%.
Still, the comparison is fair because the current collector is not tuned,
either.

The {\tt append-3} benchmark is an implementation of {\tt append} which first
reverses its first argument and then goes into a tail-recursive loop to cons
the elements of this reversed list onto its second argument.

In general the new system is faster than the older; the most dramatic
speedup (102\%) is observed in the recursive {\tt append} benchmark, where
the old system's stack cache made performance very poor. The new stack cache
performs much better. All allocation-intensive benchmarks except {\tt boyer}
perform better with the new collector. The side-effect intensive {\tt
mergesort} benchmark performs much better than previously, attesting to the
effectiveness of the transaction recording scheme as currently implemented.

The only slowdown is observed in the {\tt puzzle} benchmark. This benchmark
references many global variables; it is possible that the slowdown is due to
the benchmark being compiled with global variable checking turned on, while
the benchmarks previously were compiled with such checking turned off. It is
also possible that the benchmark was previously compiled with generation
checking in-line; the benchmark was compiled for the new system to perform
all generation checks out-of-line.

\section{Future work}

While the current system is much nicer than the old one, there's still
plenty to do:

\begin{itemize}
\item
Generic arithmetic has not been tested well, and bignum division is thought
to be buggy. Many of the standard numeric procedures are not in place.

\item
Since bignum division is not in place, neither is {\tt bellerophon}, so
there is no flonum input support.

\item
The new collector must be tuned properly. Some work has been done here using
the {\tt 10perm8} benchmark, but there is more to do: that benchmark is not
typical, and a number of policy issues are still up in the air.

\item
We must replace the assembler. The current one is both slow and much too
forgiving; clearly a one-pass assembler with error checking can and should
be implemented. This will probably take a couple of days.

The peephole optimizer should perhaps not be hand-coded, but automatically
generated from a table. The current hand-coded one is hard to read because
it takes a stab at being efficient.

\item
The freed-up registers should be utilized for something useful, like more
hardware registers. This may introduce bugs in the assembler, so the assembler
should be fixed first.

\item
There are still some private global variables in the context switching code
and in the generic arithmetic; these should be moved into the globals
table.

\item
The bootstrap heap dumper should be modified. Currently it takes forever
to dump a heap because it keeps the image in memory rather than flushing
it to a file as it goes along. This is easy to fix.

\item
Sundry compiler bugs should be fixed: there is the stack cache/spill problem;
and we need a new macro expander which creates better code for named {\tt LET}
and which supports quasiquotations.

\item
A new interpreter would be nice, as the current one is very slow. However,
when the compiler runs inside Larceny that problem is rather moot.

\item
The development environment should become part of Larceny.

\item
We need to add support for the static area both in the heap loader and the
heap dumper.

\end{itemize}

\newcommand{\xx}{\verb+   +}  % Don't knock it, it works.

\begin{figure}[h]
\begin{center}
\begin{tabular}{|l|c|c|c|}
\hline
                            & Larceny  & Larceny   & Speedup    \\
                            &  v0.19   & v0.20     &   (\%)      \\
\hline\hline
{\tt fib30}                 &   6.3    &    6.2    &   1.6       \\
{\tt cpstak}                &   0.38   &    0.32   &  18.8       \\
{\tt reverse}               &&&\\
\xx     $10000 \times 100$  &   1.8    &    1.6    &  12.5       \\
\xx     $100 \times 10000$  &   3.3    &    2.8    &  17.9       \\
{\tt reverse!}              &&&\\
\xx     $10000 \times 100$  &   1.4    &    1.4    &   0          \\
\xx     $100 \times 10000$  &   1.5    &    1.4    &   7.1        \\
{\tt append-1}              &&&\\
\xx     $10000 \times 100$  &   3.9    &    3.7    &   5.4       \\
\xx     $100 \times 10000$  &  28.5    &    14.1   & 102.1       \\
{\tt append-2}              &&&\\
\xx     $10000 \times 100$  &   3.0    &    2.8    &   7.1       \\
\xx     $100 \times 10000$  &   4.7    &    4.0    &   17.5      \\
{\tt append-3}              &&&\\
\xx     $10000 \times 100$  &          &    3.9    &   --        \\
\xx     $100 \times 10000$  &          &    6.2    &   --        \\
{\tt idiv2}                 &   0.24   &    0.23   &   4.3       \\
{\tt rdiv2}                 &   0.45   &    0.44   &   2.3       \\
{\tt boyer}                 &   5.2    &    5.2    &   0         \\
{\tt perm8}                 &   2.6    &    2.5    &   4.0       \\
{\tt 10perm8}               &  34.0    &   29.8    &   14.1      \\
{\tt sumperms}              &   0.43   &    0.42   &   2.4       \\
{\tt mergesort}             &  10.6    &    7.7    &   37.7      \\
{\tt quick-1}               &   5.8    &    5.8    &   0         \\
{\tt recursive-nfa}         &   2.1    &    2.0    &   5.0       \\
{\tt sieve-4}               &   1.4    &    1.4    &   0         \\
{\tt puzzle}                &   2.9    &    3.1    &   -6.9      \\
\hline
\end{tabular}
\end{center}
\caption{Times in seconds on the old and new systems.}
\end{figure}

\end{document}
